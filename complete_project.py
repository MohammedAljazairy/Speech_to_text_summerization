# -*- coding: utf-8 -*-
"""complete_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZVwsXXx-IQjLBV17JQGMJ2fWnq8FEIf2

## Installing the data set from transformers datasets
"""

from IPython.display import HTML , Javascript
from google.colab.output import eval_js
import base64

js = Javascript("""
async function recordAudio() {
const div = document.createElement('div');
const audio = document.createElement('audio');
const strtButton = document.createElement('button');
const stopButton = document.createElement('button');
strtButton.textContent="Start Recording";
stopButton.textContent="Stop Recording";
document.body.appendChild(div);
div.appendChild(strtButton);
div.appendChild(audio);
const stream =await navigator.mediaDevices.getUserMedia({audio:true});
let recorder=new MediaRecorder(stream);
audio.style.display='block';
audio.srcObject= stream;
audio.controls=true;
audio.muted = true;
await new Promise((resolve) => strtButton.onclick = resolve);
strtButton.replaceWith(stopButton);
recorder.start();
await new Promise((resolve) => stopButton.onclick = resolve);
recorder.stop();
let recData=await new Promise((resolve)=> recorder.ondataavailable = resolve);
let arrBuff=await recData.data.arrayBuffer();
stream.getAudioTracks()[0].stop();
div.remove()

let binaryString='';
let bytes = new Uint8Array(arrBuff);
bytes.forEach((byte)=>{binaryString+= String.fromCharCode(byte)});
const url =URL.createObjectURL(recData.data);
const player=document.createElement('audio');
player.controls=true;
player.src=url;
document.body.appendChild(player);
return btoa(binaryString)};
""")

display(js)
output = eval_js('recordAudio({})')
with open('audio.wav','wb') as file:
  binary =base64.b64decode(output)
  file.write(binary)
  print('Recording saved to :',file.name)

pip install -U openai-whisper

import whisper
import os
model = whisper.load_model('medium')
result = model.transcribe('audio.wav' , fp16=False)
print(result['text'])

pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q

!nvidia-smi

from transformers import pipeline, set_seed

import matplotlib.pyplot as plt
from datasets import load_dataset
import pandas as pd
from datasets import load_dataset, load_metric

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

import nltk
from nltk.tokenize import sent_tokenize

from tqdm import tqdm
import torch

nltk.download("punkt")

"""## Check if we are using GPU or not"""

device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
model_ckpt = "google/pegasus-cnn_dailymail"

tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

# !pip install transformers[torch]

model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)

"""## Load data"""

dataset_samsum = load_dataset('samsum')

dataset_samsum

split_lengths = [len(dataset_samsum[split]) for split in dataset_samsum]
split_lengths

print(f"Features: {dataset_samsum['train'].column_names}")

print("Dialogue:")
print(dataset_samsum["test"][1]['dialogue'])
print("Summary:")
print(dataset_samsum["test"][1]['summary'])

print("Dialogue:")
print(dataset_samsum["test"][0]['dialogue'])
print("Summary:")
print(dataset_samsum["test"][0]['summary'])

dialogue = dataset_samsum["test"][0]['dialogue']
dialogue

pipe = pipeline('summarization', model=model_ckpt)

pipe_out = pipe(dialogue)
pipe_out

print(pipe_out[0]['summary_text'].replace(" .<n>", ".\n"))

def generate_batch_sized_chunks(list_of_elements, batch_size):
  """Split the dataset into smaller batches that we can process simultaneously"""
  for i in range(0, len(list_of_elements), batch_size):
    yield list_of_elements[i: i+batch_size]


def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,
                               batch_size=16, device=device,
                               column_text="article",
                               column_summary="highlights"):
    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))
    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))

    for article_batch, target_batch in tqdm(
        zip(article_batches, target_batches), total=len(article_batches)):

        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,
                        padding="max_length", return_tensors="pt")

        summaries = model.generate(input_ids=inputs["input_ids"].to(device),
                         attention_mask=inputs["attention_mask"].to(device),
                         length_penalty=0.8, num_beams=8, max_length=128)
        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''

        # Finally, we decode the generated texts,
        # replace the  token, and add the decoded texts with the references to the metric.
        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,
                                clean_up_tokenization_spaces=True)
               for s in summaries]

        decoded_summaries = [d.replace("", " ") for d in decoded_summaries]


        metric.add_batch(predictions=decoded_summaries, references=target_batch)

    #  Finally compute and return the ROUGE scores.
    score = metric.compute()
    return score

# from google.colab import drive
# drive.mount('/content/drive')

# # Define the path to save the results file
# results_file_path = "/content/drive/MyDrive/saving_text_summarization/results.txt"

# # Calculate the metric
# rouge_metric = load_metric('rouge')
# score = calculate_metric_on_test_ds(dataset_samsum['test'], rouge_metric, model_pegasus, tokenizer, column_text='dialogue', column_summary='summary', batch_size=8)

# # Save the results to a file
# with open(results_file_path, 'w') as file:
#     file.write(str(score))

# from google.colab import drive
# drive.mount('/content/drive')

# # Define the path to the saved results file
# results_file_path = "/content/drive/MyDrive/saving_text_summarization/results.txt"

# # Load the results from the file
# with open(results_file_path, 'r') as file:
#     saved_score = file.read()

# # Print or use the loaded results
# print(saved_score)

rouge_metric = load_metric('rouge')
score = calculate_metric_on_test_ds(dataset_samsum['test'], rouge_metric, model_pegasus, tokenizer, column_text='dialogue', column_summary='summary', batch_size=8)

saved_torch = torch.save(model_pegasus.state_dict(), 'pegasus_model.pt')

# This is new data for the model
# We are printing the accuracy of this model on this data
rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)
pd.DataFrame(rouge_dict, index=['pegasus'])

def convert_examples_to_features(example_batch):
    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )

    with tokenizer.as_target_tokenizer():
        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )

    return {
        'input_ids' : input_encodings['input_ids'],
        'attention_mask': input_encodings['attention_mask'],
        'labels': target_encodings['input_ids']
    }

dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)

# Now the training data is represented as numerical
dataset_samsum_pt['train'][0]

from transformers import DataCollatorForSeq2Seq
seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/saving_text_summarization

from google.colab import drive
drive.mount('/content/drive')
output_dir = r"D:/AAST/Semester 7/Neural Network/Project/Saved_text_summarization_model"
# output_dir = r"/content/drive/MyDrive/saving_text_summarization"
from google.colab import files
import shutil

# Zip the output directory
shutil.make_archive("/content/saving_text_summarization", 'zip', output_dir)

# Download the zipped file
files.download("/content/saving_text_summarization.zip")

from transformers import TrainingArguments, Trainer

trainer_args = TrainingArguments(
    output_dir=output_dir, num_train_epochs=2, warmup_steps=500,
    per_device_train_batch_size=1, per_device_eval_batch_size=1,
    weight_decay=0.01, logging_steps=10,
    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,
    gradient_accumulation_steps=16
)

trainer = Trainer(model=model_pegasus, args=trainer_args,
                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,
                  train_dataset=dataset_samsum_pt["train"],
                  eval_dataset=dataset_samsum_pt["validation"])

# saved_model_name = 'saving_text_summarization'
# file_path = r"/content/drive/MyDrive/saving_text_summarization"
# checkpoint_file_path = "/content/drive/MyDrive/saving_text_summarization/pytorch_model.bin"
# trainer.train(resume_from_checkpoint=True, trial=True)
output_dir = r"D:/AAST/Semester 7/Neural Network/Project/model_checkpoint"
# trainer.save_checkpoint(output_dir, "model_checkpoint")
trainer.train()
trainer.save_state()
trainer.save_model(trainer_args.output_dir)

# if list(pathlib.Path(trainer_args.output_dir).glob("checkpoint-*")):
#   trainer.train(resume_from_checkpoint=True)
# else:
#   trainer.train()
# trainer.save_state()
# trainer.save_model(training_args.output_dir)

score = calculate_metric_on_test_ds(
    dataset_samsum['test'], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'
)

rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )

pd.DataFrame(rouge_dict, index = [f'pegasus'] )

model_pegasus.save_pretrained("pegasus-samsum-model")

tokenizer.save_pretrained("tokenizer")

"""## Test"""

dataset_samsum = load_dataset("samsum")

tokenizer = AutoTokenizer.from_pretrained("tokenizer")

sample_text = dataset_samsum["test"][0]["dialogue"]

reference = dataset_samsum["test"][0]["summary"]

gen_kwargs = {"length_penalty": 0.8, "num_beams":8, "max_length": 128}

pipe = pipeline("summarization", model="pegasus-samsum-model",tokenizer=tokenizer)

print("Dialogue:")
print(sample_text)


print("\nReference Summary:")
print(reference)


print("\nModel Summary:")
print(pipe(sample_text, **gen_kwargs)[0]["summary_text"])

print(pipe(result['text'], **gen_kwargs)[0]["summary_text"])